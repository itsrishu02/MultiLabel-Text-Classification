{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Multi-Label Text Classification Dataset.csv\")\n",
    "df_data = proc_data = data.drop([\"meshMajor\",\"pmid\",\"meshid\",\"meshroot\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.sample(n = 50000, random_state = 4)\n",
    "# Dropping null row\n",
    "df_data = df_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and abstract to increase power\n",
    "df_data[\"mixed\"] = df_data[\"Title\"] + \". \" + df_data[\"abstractText\"]\n",
    "df_data.drop(columns=[\"abstractText\", \"Title\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>Z</th>\n",
       "      <th>mixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16477</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ocular findings in subacute sclerosing panence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Notch signaling suppresses IgH gene expression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46459</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Immunological and physiological effects of chr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       A  B  C  D  E  F  G  H  I  J  L  M  N  Z  \\\n",
       "16477  1  1  1  0  1  0  0  0  0  0  0  1  1  0   \n",
       "5969   1  1  0  1  0  0  1  0  0  0  1  0  0  0   \n",
       "46459  0  1  1  1  1  0  1  0  0  0  0  0  1  0   \n",
       "\n",
       "                                                   mixed  \n",
       "16477  Ocular findings in subacute sclerosing panence...  \n",
       "5969   Notch signaling suppresses IgH gene expression...  \n",
       "46459  Immunological and physiological effects of chr...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RISHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>Z</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16477</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ocular findings subacute sclerosing panencepha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>notch signaling suppresses igh gene expression...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       A  B  C  D  E  F  G  H  I  J  L  M  N  Z  \\\n",
       "16477  1  1  1  0  1  0  0  0  0  0  0  1  1  0   \n",
       "5969   1  1  0  1  0  0  1  0  0  0  1  0  0  0   \n",
       "\n",
       "                                                combined  \n",
       "16477  ocular findings subacute sclerosing panencepha...  \n",
       "5969   notch signaling suppresses igh gene expression...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Remove single characters\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])    \n",
    "    return text\n",
    "\n",
    "\n",
    "df_data[\"combined\"] = df_data[\"mixed\"].apply(clean_text)\n",
    "df_data.drop(columns=[\"mixed\"], axis=1, inplace=True)\n",
    "\n",
    "df_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (34998, 15), Test: (13500, 15), Valid: (1500, 15)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df_data, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.10, shuffle=True)\n",
    "\n",
    "print(f\"Train: {df_train.shape}, Test: {df_test.shape}, Valid: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = list(df['combined'])\n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "          'input_ids': inputs['input_ids'].flatten(),\n",
    "          'attention_mask': inputs['attention_mask'].flatten(),\n",
    "          'targets': torch.FloatTensor(self.targets[index]),\n",
    "          'title': title\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = list(df_data.columns)\n",
    "target_list = target_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 128\n",
    "TEST_BATCH_SIZE = 16\n",
    "\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN, target_list)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DistilBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.classifier = torch.nn.Linear(768, 14)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "model = DistilBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBERTClass()\n",
    "data_dir = \"D:\\\\Sem 2\\\\NLP\\\\Assignment_3\\\\task\"\n",
    "model.load_state_dict(torch.load(os.path.join(data_dir,\"output\",\"trained_model.bin\")))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to eval mode (turn off dropout, fix batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            correct_predictions += np.sum(outputs==targets)\n",
    "            num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8844338624338625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "test_acc, test_loss = eval_model(test_data_loader, model, optimizer)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "    \"\"\"\n",
    "    Outputs:\n",
    "      predictions -\n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "\n",
    "    titles = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    target_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for data in data_loader:\n",
    "        title = data[\"title\"]\n",
    "        ids = data[\"input_ids\"].to(device, dtype = torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype = torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype = torch.float)\n",
    "\n",
    "        # outputs = model(ids, mask, token_type_ids)\n",
    "        outputs = model(ids, mask)\n",
    "        # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "        outputs = torch.sigmoid(outputs).detach().cpu()\n",
    "        # thresholding at 0.5\n",
    "        preds = outputs.round()\n",
    "        targets = targets.detach().cpu()\n",
    "\n",
    "        titles.extend(title)\n",
    "        predictions.extend(preds)\n",
    "        prediction_probs.extend(outputs)\n",
    "        target_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions)\n",
    "    prediction_probs = torch.stack(prediction_probs)\n",
    "    target_values = torch.stack(target_values)\n",
    "\n",
    "    return titles, predictions, prediction_probs, target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles:13500 \n",
      "predictions:torch.Size([13500, 14]) \n",
      "prediction_probs:torch.Size([13500, 14]) \n",
      "target_values:torch.Size([13500, 14])\n"
     ]
    }
   ],
   "source": [
    "titles, predictions, prediction_probs, target_values = get_predictions(model, test_data_loader)\n",
    "\n",
    "print(f\"titles:{len(titles)} \\npredictions:{predictions.shape} \\nprediction_probs:{prediction_probs.shape} \\ntarget_values:{target_values.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.81      0.81      0.81      6241\n",
      "           B       0.97      0.98      0.98     12559\n",
      "           C       0.91      0.86      0.89      7152\n",
      "           D       0.92      0.93      0.92      8340\n",
      "           E       0.83      0.93      0.88     10599\n",
      "           F       0.76      0.78      0.77      2359\n",
      "           G       0.84      0.89      0.86      9066\n",
      "           H       0.63      0.15      0.24      1640\n",
      "           I       0.73      0.57      0.64      1482\n",
      "           J       0.73      0.48      0.58      1435\n",
      "           L       0.69      0.50      0.58      2027\n",
      "           M       0.88      0.90      0.89      5871\n",
      "           N       0.85      0.74      0.79      6261\n",
      "           Z       0.84      0.69      0.76      2213\n",
      "\n",
      "   micro avg       0.87      0.85      0.86     77245\n",
      "   macro avg       0.81      0.73      0.76     77245\n",
      "weighted avg       0.86      0.85      0.85     77245\n",
      " samples avg       0.87      0.85      0.85     77245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RISHU\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_values, predictions, target_names=target_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Metrics:\n",
      "Class: A\n",
      "Precision: 0.8133633151300996\n",
      "Recall: 0.8114084281365166\n",
      "F1-Score: 0.8123846955963744\n",
      "\n",
      "Class: B\n",
      "Precision: 0.9701997169366252\n",
      "Recall: 0.9824826817421769\n",
      "F1-Score: 0.9763025675515291\n",
      "\n",
      "Class: C\n",
      "Precision: 0.9088771310993533\n",
      "Recall: 0.8646532438478747\n",
      "F1-Score: 0.886213814846661\n",
      "\n",
      "Class: D\n",
      "Precision: 0.9174831779010743\n",
      "Recall: 0.93189448441247\n",
      "F1-Score: 0.9246326809826899\n",
      "\n",
      "Class: E\n",
      "Precision: 0.8344449134655973\n",
      "Recall: 0.9325408057363902\n",
      "F1-Score: 0.8807699162359651\n",
      "\n",
      "Class: F\n",
      "Precision: 0.7625928984310487\n",
      "Recall: 0.7829588808817295\n",
      "F1-Score: 0.7726417067559088\n",
      "\n",
      "Class: G\n",
      "Precision: 0.836544437538844\n",
      "Recall: 0.8908007941760424\n",
      "F1-Score: 0.8628205128205129\n",
      "\n",
      "Class: H\n",
      "Precision: 0.6329787234042553\n",
      "Recall: 0.14512195121951219\n",
      "F1-Score: 0.2361111111111111\n",
      "\n",
      "Class: I\n",
      "Precision: 0.7251512532411409\n",
      "Recall: 0.5661268556005398\n",
      "F1-Score: 0.6358469117089807\n",
      "\n",
      "Class: J\n",
      "Precision: 0.7322751322751323\n",
      "Recall: 0.48222996515679445\n",
      "F1-Score: 0.5815126050420169\n",
      "\n",
      "Class: L\n",
      "Precision: 0.6876712328767123\n",
      "Recall: 0.49531327084361126\n",
      "F1-Score: 0.5758531689131058\n",
      "\n",
      "Class: M\n",
      "Precision: 0.8821458507963118\n",
      "Recall: 0.8962698007153806\n",
      "F1-Score: 0.8891517404528557\n",
      "\n",
      "Class: N\n",
      "Precision: 0.8516402506450423\n",
      "Recall: 0.7380610126177927\n",
      "F1-Score: 0.7907931890134338\n",
      "\n",
      "Class: Z\n",
      "Precision: 0.8392265193370165\n",
      "Recall: 0.6863985539990962\n",
      "F1-Score: 0.7551578424061646\n",
      "\n",
      "\n",
      "Aggregate Metrics:\n",
      "Micro Average Precision: 0.8844338624338625\n",
      "Micro Average Recall: 0.8844338624338625\n",
      "Micro Average F1-Score: 0.8844338624338625\n",
      "\n",
      "Macro Average Precision: 0.8816535974273767\n",
      "Macro Average Recall: 0.8784831340532948\n",
      "Macro Average F1-Score: 0.8799628435449589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each class\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(target_values, predictions, average=None)\n",
    "\n",
    "# Print precision, recall, and F1-score for each class\n",
    "print(\"\\nClass-wise Metrics:\")\n",
    "for i, label in enumerate(['A','B','C','D','E','F','G','H','I','J','L','M','N','Z']):\n",
    "    print(f\"Class: {label}\")\n",
    "    print(f\"Precision: {precision[i]}\")\n",
    "    print(f\"Recall: {recall[i]}\")\n",
    "    print(f\"F1-Score: {f1[i]}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate micro and macro average precision, recall, and F1-score\n",
    "micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(target_values.ravel(), predictions.ravel(), average='micro')\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(target_values.ravel(), predictions.ravel(), average='macro')\n",
    "\n",
    "\n",
    "# Print aggregate metrics\n",
    "print(\"\\nAggregate Metrics:\")\n",
    "print(f\"Micro Average Precision: {micro_precision}\")\n",
    "print(f\"Micro Average Recall: {micro_recall}\")\n",
    "print(f\"Micro Average F1-Score: {micro_f1}\")\n",
    "print(\"\")\n",
    "print(f\"Macro Average Precision: {macro_precision}\")\n",
    "print(f\"Macro Average Recall: {macro_recall}\")\n",
    "print(f\"Macro Average F1-Score: {macro_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
